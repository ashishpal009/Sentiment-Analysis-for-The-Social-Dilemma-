{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d89743e3-5024-468a-824f-2e03231074f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63af0295-e623-45a1-8f0e-349810a52746",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"C:\\Users\\ASHISH PAL\\Downloads\\DL + AI Exam Paper\\DL + AI Exam Paper\\Dataset\\RNN\\tweets_train.csv\")\n",
    "test_data = pd.read_csv(r\"C:\\Users\\ASHISH PAL\\Downloads\\DL + AI Exam Paper\\DL + AI Exam Paper\\Dataset\\RNN\\tweets_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca0f2ab3-7631-4b24-8bf7-fd409e2006cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Romy üëë</td>\n",
       "      <td>Bolton - England</td>\n",
       "      <td>Tables turn, bridges burn, you live and learn.</td>\n",
       "      <td>2009-06-15 09:00:39</td>\n",
       "      <td>525</td>\n",
       "      <td>896</td>\n",
       "      <td>3854</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-19 15:19:32</td>\n",
       "      <td>['TheSocialDilemma']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "      <td>TheSocialDilemma is an eye opener isn t it ple...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TLynn Peterson</td>\n",
       "      <td>Black Canyon City, Arizona</td>\n",
       "      <td>Acquired disability ‚ôø after an accident. Livin...</td>\n",
       "      <td>2013-05-29 00:17:46</td>\n",
       "      <td>5045</td>\n",
       "      <td>5374</td>\n",
       "      <td>48152</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-13 00:31:46</td>\n",
       "      <td>['TheSocialDilemma']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "      <td>TheSocialDilemma If we don t agree on what is ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rutger Kosters</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Cloud Solution Architect @NetApp | VCDX #209 |...</td>\n",
       "      <td>2014-10-07 09:34:10</td>\n",
       "      <td>574</td>\n",
       "      <td>251</td>\n",
       "      <td>1070</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-09 21:10:33</td>\n",
       "      <td>['TheSocialDilemma']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "      <td>Watching TheSocialDilemma scary to see social ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IDFWUüóØüáøüáºüáøüá¶</td>\n",
       "      <td>East London, South Africa</td>\n",
       "      <td>YOU HAVE OPTIONS, YOU CAN‚ÄôT JUST TAKE WHAT LIF...</td>\n",
       "      <td>2012-08-22 11:09:06</td>\n",
       "      <td>706</td>\n",
       "      <td>618</td>\n",
       "      <td>1320</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-11 04:33:08</td>\n",
       "      <td>No hashtags</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>False</td>\n",
       "      <td>You check your social media before you pee in ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Treebel</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>No description</td>\n",
       "      <td>2010-09-25 07:25:31</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>55</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-17 16:27:38</td>\n",
       "      <td>['thesocialdilemma']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>False</td>\n",
       "      <td>watch thesocialdilemma and see what s actually...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16995</th>\n",
       "      <td>Ra'Chelle Rogers</td>\n",
       "      <td>Philadelphia/New York</td>\n",
       "      <td>mom. media. art. Publicist. Philly. Opinions e...</td>\n",
       "      <td>2009-05-22 13:34:14</td>\n",
       "      <td>387</td>\n",
       "      <td>400</td>\n",
       "      <td>1274</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-13 22:12:29</td>\n",
       "      <td>['TheSocialDilemma']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>False</td>\n",
       "      <td>Watching TheSocialDilemma</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16996</th>\n",
       "      <td>Mike Lynch</td>\n",
       "      <td>Boston, MA, USA</td>\n",
       "      <td>higher education professional . educator . doc...</td>\n",
       "      <td>2009-04-27 01:16:09</td>\n",
       "      <td>4444</td>\n",
       "      <td>4995</td>\n",
       "      <td>52692</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-20 15:17:14</td>\n",
       "      <td>['TheSocialDilemma']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>False</td>\n",
       "      <td>If you re not paying for the product you are t...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16997</th>\n",
       "      <td>James Baldwin was a genius.</td>\n",
       "      <td>Terra-Belle, USA</td>\n",
       "      <td>Class based policies won't fix the racial weal...</td>\n",
       "      <td>2012-10-05 23:27:21</td>\n",
       "      <td>879</td>\n",
       "      <td>1456</td>\n",
       "      <td>50103</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-19 14:04:32</td>\n",
       "      <td>['TheSocialDilemma']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "      <td>Watching TheSocialDilemma Let s see</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16998</th>\n",
       "      <td>Johan - SocialMediaBreakup</td>\n",
       "      <td>Dublin City, Ireland</td>\n",
       "      <td>Helping people to beat their compulsive phone ...</td>\n",
       "      <td>2018-10-25 22:21:17</td>\n",
       "      <td>1009</td>\n",
       "      <td>1283</td>\n",
       "      <td>19290</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-18 15:37:00</td>\n",
       "      <td>['TheSocialDilemma']</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes TheSocialDilemma what a perfect time to be...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16999</th>\n",
       "      <td>attilacsordas</td>\n",
       "      <td>Cambridge, UK</td>\n",
       "      <td>healthy longevity startupper/biologist/philoso...</td>\n",
       "      <td>2007-03-27 09:26:42</td>\n",
       "      <td>4278</td>\n",
       "      <td>558</td>\n",
       "      <td>3844</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-09-19 18:51:11</td>\n",
       "      <td>['TheSocialDilemma']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "      <td>had the impression that is actually sitting in...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16992 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         user_name               user_location  \\\n",
       "0                           Romy üëë            Bolton - England   \n",
       "1                   TLynn Peterson  Black Canyon City, Arizona   \n",
       "2                   Rutger Kosters                 Netherlands   \n",
       "3                       IDFWUüóØüáøüáºüáøüá¶   East London, South Africa   \n",
       "4                          Treebel                     Unknown   \n",
       "...                            ...                         ...   \n",
       "16995             Ra'Chelle Rogers       Philadelphia/New York   \n",
       "16996                   Mike Lynch             Boston, MA, USA   \n",
       "16997  James Baldwin was a genius.            Terra-Belle, USA   \n",
       "16998   Johan - SocialMediaBreakup        Dublin City, Ireland   \n",
       "16999                attilacsordas               Cambridge, UK   \n",
       "\n",
       "                                        user_description         user_created  \\\n",
       "0         Tables turn, bridges burn, you live and learn.  2009-06-15 09:00:39   \n",
       "1      Acquired disability ‚ôø after an accident. Livin...  2013-05-29 00:17:46   \n",
       "2      Cloud Solution Architect @NetApp | VCDX #209 |...  2014-10-07 09:34:10   \n",
       "3      YOU HAVE OPTIONS, YOU CAN‚ÄôT JUST TAKE WHAT LIF...  2012-08-22 11:09:06   \n",
       "4                                         No description  2010-09-25 07:25:31   \n",
       "...                                                  ...                  ...   \n",
       "16995  mom. media. art. Publicist. Philly. Opinions e...  2009-05-22 13:34:14   \n",
       "16996  higher education professional . educator . doc...  2009-04-27 01:16:09   \n",
       "16997  Class based policies won't fix the racial weal...  2012-10-05 23:27:21   \n",
       "16998  Helping people to beat their compulsive phone ...  2018-10-25 22:21:17   \n",
       "16999  healthy longevity startupper/biologist/philoso...  2007-03-27 09:26:42   \n",
       "\n",
       "       user_followers  user_friends  user_favourites  user_verified  \\\n",
       "0                 525           896             3854          False   \n",
       "1                5045          5374            48152          False   \n",
       "2                 574           251             1070          False   \n",
       "3                 706           618             1320          False   \n",
       "4                   9            13               55          False   \n",
       "...               ...           ...              ...            ...   \n",
       "16995             387           400             1274          False   \n",
       "16996            4444          4995            52692          False   \n",
       "16997             879          1456            50103          False   \n",
       "16998            1009          1283            19290          False   \n",
       "16999            4278           558             3844          False   \n",
       "\n",
       "                      date              hashtags               source  \\\n",
       "0      2020-09-19 15:19:32  ['TheSocialDilemma']  Twitter for Android   \n",
       "1      2020-09-13 00:31:46  ['TheSocialDilemma']  Twitter for Android   \n",
       "2      2020-09-09 21:10:33  ['TheSocialDilemma']  Twitter for Android   \n",
       "3      2020-09-11 04:33:08           No hashtags  Twitter for Android   \n",
       "4      2020-09-17 16:27:38  ['thesocialdilemma']   Twitter for iPhone   \n",
       "...                    ...                   ...                  ...   \n",
       "16995  2020-09-13 22:12:29  ['TheSocialDilemma']   Twitter for iPhone   \n",
       "16996  2020-09-20 15:17:14  ['TheSocialDilemma']   Twitter for iPhone   \n",
       "16997  2020-09-19 14:04:32  ['TheSocialDilemma']      Twitter Web App   \n",
       "16998  2020-09-18 15:37:00  ['TheSocialDilemma']            TweetDeck   \n",
       "16999  2020-09-19 18:51:11  ['TheSocialDilemma']      Twitter Web App   \n",
       "\n",
       "       is_retweet                                         clean_text Sentiment  \n",
       "0           False  TheSocialDilemma is an eye opener isn t it ple...   Neutral  \n",
       "1           False  TheSocialDilemma If we don t agree on what is ...  Positive  \n",
       "2           False  Watching TheSocialDilemma scary to see social ...  Negative  \n",
       "3           False  You check your social media before you pee in ...  Positive  \n",
       "4           False  watch thesocialdilemma and see what s actually...  Negative  \n",
       "...           ...                                                ...       ...  \n",
       "16995       False                          Watching TheSocialDilemma   Neutral  \n",
       "16996       False  If you re not paying for the product you are t...   Neutral  \n",
       "16997       False                Watching TheSocialDilemma Let s see   Neutral  \n",
       "16998       False  Yes TheSocialDilemma what a perfect time to be...  Positive  \n",
       "16999       False  had the impression that is actually sitting in...  Negative  \n",
       "\n",
       "[16992 rows x 14 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb43b4dd-159e-4b4c-bf40-109ef3e19850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ASHISH\n",
      "[nltk_data]     PAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19c0955e-5c6b-49a2-ab01-4c66768d5be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ASHISH\n",
      "[nltk_data]     PAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e15f452c-c424-4d85-999f-f1a3c56470c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by lowercasing, removing URLs, special characters,\n",
    "    and stopwords.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):  # Check if input is not a string\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Rejoin the tokens\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60028437-31c7-4b3a-9591-e173f7eb407a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASHISH PAL\\AppData\\Local\\Temp\\ipykernel_23288\\4280027138.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['clean_text'] = train_data['clean_text'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "train_data['clean_text'] = train_data['clean_text'].apply(preprocess_text)\n",
    "test_data['clean_text'] = test_data['clean_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a5638bc-032e-4a41-8a7e-d2d2c687b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASHISH PAL\\AppData\\Local\\Temp\\ipykernel_23288\\2723459011.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['Sentiment'] = label_encoder.fit_transform(train_data['Sentiment'])\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Encode Sentiments\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['Sentiment'] = label_encoder.fit_transform(train_data['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c889c251-7a2d-444c-ba63-2e444c68866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(num_words=10000)  # Use top 10,000 words\n",
    "tokenizer.fit_on_texts(train_data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67b70695-8449-4af3-865c-ee11c313a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train_data['clean_text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22c07615-f861-4e90-958e-3e3bfa5f6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50  # Adjust based on the dataset\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c09bf771-eb46-4e6f-8397-937a943efdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode target variable\n",
    "y_train = pd.get_dummies(train_data['Sentiment']).values\n",
    "\n",
    "# Step 7: Train-Test Split for Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf06df54-c7bb-4f46-a9a3-69f7524cac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Build the RNN Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_len),\n",
    "    SimpleRNN(128, return_sequences=False),  # RNN layer\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # Output layer for 3 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f42ba7c-5772-429b-bb19-6a553da75cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Compile the Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91753a8d-eb8a-4ae0-9591-04254a13f1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.6367 - loss: 0.8191 - val_accuracy: 0.8347 - val_loss: 0.4508\n",
      "Epoch 2/10\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.9115 - loss: 0.2654 - val_accuracy: 0.8388 - val_loss: 0.4606\n",
      "Epoch 3/10\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9633 - loss: 0.1107 - val_accuracy: 0.8358 - val_loss: 0.5906\n",
      "Epoch 4/10\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9821 - loss: 0.0564 - val_accuracy: 0.8282 - val_loss: 0.6735\n",
      "Epoch 5/10\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9885 - loss: 0.0396 - val_accuracy: 0.8205 - val_loss: 0.7565\n",
      "Epoch 6/10\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.9908 - loss: 0.0322 - val_accuracy: 0.8288 - val_loss: 0.8213\n",
      "Epoch 7/10\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.9937 - loss: 0.0218 - val_accuracy: 0.8299 - val_loss: 0.8697\n",
      "Epoch 8/10\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.9919 - loss: 0.0267 - val_accuracy: 0.8070 - val_loss: 0.8733\n",
      "Epoch 9/10\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.9935 - loss: 0.0214 - val_accuracy: 0.8332 - val_loss: 0.8879\n",
      "Epoch 10/10\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9946 - loss: 0.0156 - val_accuracy: 0.8129 - val_loss: 0.9831\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "756e6061-6e5a-4b66-a8cc-95acc0e5d864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a106eb4-00a9-4d2b-8624-d7679624ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Predicted_Sentiment'] = label_encoder.inverse_transform(y_pred_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57bbd1e7-e578-4937-9639-3333feca0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['clean_text', 'Predicted_Sentiment']].to_csv(\"Test_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00ea3e5e-e6ca-4505-8ec8-b75af11bcdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'Test_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions saved to 'Test_predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb5b8f89-591a-4ba7-96a1-add643edc933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file with predictions saved to 'Test_data_with_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step: Decode predictions back to original sentiment labels\n",
    "test_data['Predicted_Sentiment'] = label_encoder.inverse_transform(y_pred_labels)\n",
    "\n",
    "# Step: Save the updated test file with predictions\n",
    "test_data.to_csv(\"Test_data_with_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3301b-2f79-4e5e-9d2f-d90a0b100485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
